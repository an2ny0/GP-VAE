{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Physionet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZtDdWOq5WaK"
      },
      "source": [
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write('absl-py==0.7.0\\n')\n",
        "    f.write('numpy==1.16.4\\n')\n",
        "    f.write('scipy==1.2.0\\n')\n",
        "    f.write('tensorflow==1.15.0\\n')\n",
        "    f.write('tensorflow-gpu==1.15.0\\n')\n",
        "    f.write('tensorflow_probability==0.7.0\\n')\n",
        "    f.write('matplotlib\\n')\n",
        "    f.write('scikit-learn')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "--Ccx6rL4f7U",
        "outputId": "b70e5dba-0e3f-404f-8037-92fd25ac88ca"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting absl-py==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/bc/ab68120d1d89ae23b694a55fe2aece2f91194313b71f9b05a80b32d3c24b/absl-py-0.7.0.tar.gz (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.3MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/d1/45be1144b03b6b1e24f9a924f23f66b4ad030d834ad31fb9e5581bd328af/numpy-1.16.4-cp37-cp37m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 178kB/s \n",
            "\u001b[?25hCollecting scipy==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/39/066ecde98f373430bf7a39a02d91c7075b01ef4fc928456e8e31577342d6/scipy-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (26.6MB)\n",
            "\u001b[K     |████████████████████████████████| 26.6MB 123kB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 42kB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/72/d06017379ad4760dc58781c765376ce4ba5dcf3c08d37032eeefbccf1c51/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 42kB/s \n",
            "\u001b[?25hCollecting tensorflow_probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py==0.7.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 4)) (1.34.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 4)) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 4)) (0.36.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 4)) (0.8.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 4)) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0->-r requirements.txt (line 4)) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_probability==0.7.0->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow_probability==0.7.0->-r requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0->-r requirements.txt (line 4)) (56.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 4)) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0->-r requirements.txt (line 4)) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 4)) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0->-r requirements.txt (line 4)) (3.4.1)\n",
            "Building wheels for collected packages: absl-py, gast\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.7.0-cp37-none-any.whl size=113530 sha256=4ec81b1a79d445315d547f453321505c9abd09db9d6b281103a6f2e1490473e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/db/f8/2c3101f72ef1ad434e4662853174126ce30201a3e163dcbeca\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=958b768aced1d2fadf030366058bd6b1f78ad5c069f7f028af212e94d1442cf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built absl-py gast\n",
            "\u001b[31mERROR: xarray 0.18.2 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-metadata 1.0.0 has requirement absl-py<0.13,>=0.9, but you'll have absl-py 0.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyerfa 2.0.0 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: absl-py, numpy, scipy, keras-applications, tensorflow-estimator, gast, tensorboard, tensorflow, tensorflow-gpu, tensorflow-probability\n",
            "  Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "  Found existing installation: tensorflow-probability 0.12.1\n",
            "    Uninstalling tensorflow-probability-0.12.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.12.1\n",
            "Successfully installed absl-py-0.7.0 gast-0.2.2 keras-applications-1.0.8 numpy-1.16.4 scipy-1.2.0 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0 tensorflow-probability-0.7.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl",
                  "gast",
                  "numpy",
                  "scipy",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_probability"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CohoYB848trx"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow_probability import distributions as tfd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib\n",
        "from random import randint\n",
        "%matplotlib inline\n",
        "matplotlib.use(\"Agg\")\n",
        "from matplotlib import pyplot as plt\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcwms3MjVWLQ"
      },
      "source": [
        "### NN utils ###\n",
        "def make_nn(output_size, hidden_sizes):\n",
        "    \"\"\" Creates fully connected neural network\n",
        "            :param output_size: output dimensionality\n",
        "            :param hidden_sizes: tuple of hidden layer sizes.\n",
        "                                 The tuple length sets the number of hidden layers.\n",
        "    \"\"\"\n",
        "    layers = [tf.keras.layers.Dense(h, activation=tf.nn.relu, dtype=tf.float32)\n",
        "              for h in hidden_sizes]\n",
        "    layers.append(tf.keras.layers.Dense(output_size, dtype=tf.float32))\n",
        "    return tf.keras.Sequential(layers)\n",
        "\n",
        "\n",
        "def make_cnn(output_size, hidden_sizes, kernel_size=3):\n",
        "    \"\"\" Construct neural network consisting of\n",
        "          one 1d-convolutional layer that utilizes temporal dependences,\n",
        "          fully connected network\n",
        "        :param output_size: output dimensionality\n",
        "        :param hidden_sizes: tuple of hidden layer sizes.\n",
        "                             The tuple length sets the number of hidden layers.\n",
        "        :param kernel_size: kernel size for convolutional layer\n",
        "    \"\"\"\n",
        "    cnn_layer = [tf.keras.layers.Conv1D(hidden_sizes[0], kernel_size=kernel_size,\n",
        "                                        padding=\"same\", dtype=tf.float32)]\n",
        "    layers = [tf.keras.layers.Dense(h, activation=tf.nn.relu, dtype=tf.float32)\n",
        "              for h in hidden_sizes[1:]]\n",
        "    layers.append(tf.keras.layers.Dense(output_size, dtype=tf.float32))\n",
        "    return tf.keras.Sequential(cnn_layer + layers)\n",
        "\n",
        "\n",
        "def make_2d_cnn(output_size, hidden_sizes, kernel_size=3):\n",
        "    \"\"\" Creates fully convolutional neural network.\n",
        "        Used as CNN preprocessor for image data (HMNIST, SPRITES)\n",
        "        :param output_size: output dimensionality\n",
        "        :param hidden_sizes: tuple of hidden layer sizes.\n",
        "                             The tuple length sets the number of hidden layers.\n",
        "        :param kernel_size: kernel size for convolutional layers\n",
        "    \"\"\"\n",
        "    layers = [tf.keras.layers.Conv2D(h, kernel_size=kernel_size, padding=\"same\",\n",
        "                                     activation=tf.nn.relu, dtype=tf.float32)\n",
        "              for h in hidden_sizes + [output_size]]\n",
        "    return tf.keras.Sequential(layers)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09QwNowtVzEP"
      },
      "source": [
        "### Utils ###\n",
        "def reduce_logmeanexp(x, axis, eps=1e-5):\n",
        "    \"\"\"Numerically-stable (?) implementation of log-mean-exp.\n",
        "    Args:\n",
        "        x: The tensor to reduce. Should have numeric type.\n",
        "        axis: The dimensions to reduce. If `None` (the default),\n",
        "              reduces all dimensions. Must be in the range\n",
        "              `[-rank(input_tensor), rank(input_tensor)]`.\n",
        "        eps: Floating point scalar to avoid log-underflow.\n",
        "    Returns:\n",
        "        log_mean_exp: A `Tensor` representing `log(Avg{exp(x): x})`.\n",
        "    \"\"\"\n",
        "    x_max = tf.reduce_max(x, axis=axis, keepdims=True)\n",
        "    return tf.log(tf.reduce_mean(\n",
        "            tf.exp(x - x_max), axis=axis, keepdims=True) + eps) + x_max\n",
        "\n",
        "\n",
        "def multiply_tfd_gaussians(gaussians):\n",
        "    \"\"\"Multiplies two tfd.MultivariateNormal distributions.\"\"\"\n",
        "    mus = [gauss.mean() for gauss in gaussians]\n",
        "    Sigmas = [gauss.covariance() for gauss in gaussians]\n",
        "    mu_3, Sigma_3, _ = multiply_gaussians(mus, Sigmas)\n",
        "    return tfd.MultivariateNormalFullCovariance(loc=mu_3, covariance_matrix=Sigma_3)\n",
        "\n",
        "\n",
        "def multiply_inv_gaussians(mus, lambdas):\n",
        "    \"\"\"Multiplies a series of Gaussians that is given as a list of mean vectors and a list of precision matrices.\n",
        "    mus: list of mean with shape [n, d]\n",
        "    lambdas: list of precision matrices with shape [n, d, d]\n",
        "    Returns the mean vector, covariance matrix, and precision matrix of the product\n",
        "    \"\"\"\n",
        "    assert len(mus) == len(lambdas)\n",
        "    batch_size = int(mus[0].shape[0])\n",
        "    d_z = int(lambdas[0].shape[-1])\n",
        "    identity_matrix = tf.reshape(tf.tile(tf.eye(d_z), [batch_size,1]), [-1,d_z,d_z])\n",
        "    lambda_new = tf.reduce_sum(lambdas, axis=0) + identity_matrix\n",
        "    mus_summed = tf.reduce_sum([tf.einsum(\"bij, bj -> bi\", lamb, mu)\n",
        "                                for lamb, mu in zip(lambdas, mus)], axis=0)\n",
        "    sigma_new = tf.linalg.inv(lambda_new)\n",
        "    mu_new = tf.einsum(\"bij, bj -> bi\", sigma_new, mus_summed)\n",
        "    return mu_new, sigma_new, lambda_new\n",
        "\n",
        "\n",
        "def multiply_inv_gaussians_batch(mus, lambdas):\n",
        "    \"\"\"Multiplies a series of Gaussians that is given as a list of mean vectors and a list of precision matrices.\n",
        "    mus: list of mean with shape [..., d]\n",
        "    lambdas: list of precision matrices with shape [..., d, d]\n",
        "    Returns the mean vector, covariance matrix, and precision matrix of the product\n",
        "    \"\"\"\n",
        "    assert len(mus) == len(lambdas)\n",
        "    batch_size = mus[0].shape.as_list()[:-1]\n",
        "    d_z = lambdas[0].shape.as_list()[-1]\n",
        "    identity_matrix = tf.tile(tf.expand_dims(tf.expand_dims(tf.eye(d_z), axis=0), axis=0), batch_size+[1,1])\n",
        "    lambda_new = tf.reduce_sum(lambdas, axis=0) + identity_matrix\n",
        "    mus_summed = tf.reduce_sum([tf.einsum(\"bcij, bcj -> bci\", lamb, mu)\n",
        "                                for lamb, mu in zip(lambdas, mus)], axis=0)\n",
        "    sigma_new = tf.linalg.inv(lambda_new)\n",
        "    mu_new = tf.einsum(\"bcij, bcj -> bci\", sigma_new, mus_summed)\n",
        "    return mu_new, sigma_new, lambda_new\n",
        "\n",
        "\n",
        "def multiply_gaussians(mus, sigmas):\n",
        "    \"\"\"Multiplies a series of Gaussians that is given as a list of mean vectors and a list of covariance matrices.\n",
        "    mus: list of mean with shape [n, d]\n",
        "    sigmas: list of covariance matrices with shape [n, d, d]\n",
        "    Returns the mean vector, covariance matrix, and precision matrix of the product\n",
        "    \"\"\"\n",
        "    assert len(mus) == len(sigmas)\n",
        "    batch_size = [int(n) for n in mus[0].shape[0]]\n",
        "    d_z = int(sigmas[0].shape[-1])\n",
        "    identity_matrix = tf.reshape(tf.tile(tf.eye(d_z), [batch_size,1]), batch_size+[d_z,d_z])\n",
        "    sigma_new = identity_matrix\n",
        "    mu_new = tf.zeros((batch_size, d_z))\n",
        "    for mu, sigma in zip(mus, sigmas):\n",
        "        sigma_inv = tf.linalg.inv(sigma_new + sigma)\n",
        "        sigma_prod = tf.matmul(tf.matmul(sigma_new, sigma_inv), sigma)\n",
        "        mu_prod = (tf.einsum(\"bij,bj->bi\", tf.matmul(sigma, sigma_inv), mu_new)\n",
        "                   + tf.einsum(\"bij,bj->bi\", tf.matmul(sigma_new, sigma_inv), mu))\n",
        "        sigma_new = sigma_prod\n",
        "        mu_new = mu_prod\n",
        "    lambda_new = tf.linalg.inv(sigma_new)\n",
        "    return mu_new, sigma_new, lambda_new"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH4Z5jLYSPXN"
      },
      "source": [
        "### Encoder ###\n",
        "class BandedJointEncoder(tf.keras.Model):\n",
        "    def __init__(self, z_size, hidden_sizes=(64, 64), window_size=3, data_type=None, **kwargs):\n",
        "        \"\"\" Encoder with 1d-convolutional network and multivariate Normal posterior\n",
        "            Used by GP-VAE with proposed banded covariance matrix\n",
        "            :param z_size: latent space dimensionality\n",
        "            :param hidden_sizes: tuple of hidden layer sizes.\n",
        "                                 The tuple length sets the number of hidden layers.\n",
        "            :param window_size: kernel size for Conv1D layer\n",
        "            :param data_type: needed for some data specific modifications, e.g:\n",
        "                tf.nn.softplus is a more common and correct choice, however\n",
        "                tf.nn.sigmoid provides more stable performance on Physionet dataset\n",
        "        \"\"\"\n",
        "        super(BandedJointEncoder, self).__init__()\n",
        "        self.z_size = int(z_size)\n",
        "        self.net = make_cnn(3*z_size, hidden_sizes, window_size)\n",
        "        self.data_type = data_type\n",
        "\n",
        "    def __call__(self, x):\n",
        "        mapped = self.net(x)\n",
        "\n",
        "        batch_size = mapped.shape.as_list()[0]\n",
        "        time_length = mapped.shape.as_list()[1]\n",
        "\n",
        "        # Obtain mean and precision matrix components\n",
        "        num_dim = len(mapped.shape.as_list())\n",
        "        perm = list(range(num_dim - 2)) + [num_dim - 1, num_dim - 2]\n",
        "        mapped_transposed = tf.transpose(mapped, perm=perm)\n",
        "        mapped_mean = mapped_transposed[:, :self.z_size]\n",
        "        mapped_covar = mapped_transposed[:, self.z_size:]\n",
        "\n",
        "        # tf.nn.sigmoid provides more stable performance on Physionet dataset\n",
        "        mapped_covar = tf.nn.sigmoid(mapped_covar)\n",
        "        #mapped_covar = tf.nn.softplus(mapped_covar)\n",
        "\n",
        "        mapped_reshaped = tf.reshape(mapped_covar, [batch_size, self.z_size, 2*time_length])\n",
        "\n",
        "        dense_shape = [batch_size, self.z_size, time_length, time_length]\n",
        "        idxs_1 = np.repeat(np.arange(batch_size), self.z_size*(2*time_length-1))\n",
        "        idxs_2 = np.tile(np.repeat(np.arange(self.z_size), (2*time_length-1)), batch_size)\n",
        "        idxs_3 = np.tile(np.concatenate([np.arange(time_length), np.arange(time_length-1)]), batch_size*self.z_size)\n",
        "        idxs_4 = np.tile(np.concatenate([np.arange(time_length), np.arange(1,time_length)]), batch_size*self.z_size)\n",
        "        idxs_all = np.stack([idxs_1, idxs_2, idxs_3, idxs_4], axis=1)\n",
        "\n",
        "        # ~10x times faster on CPU then on GPU\n",
        "        with tf.device('/cpu:0'):\n",
        "            # Obtain covariance matrix from precision one\n",
        "            mapped_values = tf.reshape(mapped_reshaped[:, :, :-1], [-1])\n",
        "            prec_sparse = tf.sparse.SparseTensor(indices=idxs_all, values=mapped_values, dense_shape=dense_shape)\n",
        "            prec_sparse = tf.sparse.reorder(prec_sparse)\n",
        "            prec_tril = tf.sparse_add(tf.zeros(prec_sparse.dense_shape, dtype=tf.float32), prec_sparse)\n",
        "            eye = tf.eye(num_rows=prec_tril.shape.as_list()[-1], batch_shape=prec_tril.shape.as_list()[:-2])\n",
        "            prec_tril = prec_tril + eye\n",
        "            cov_tril = tf.linalg.triangular_solve(matrix=prec_tril, rhs=eye, lower=False)\n",
        "            cov_tril = tf.where(tf.math.is_finite(cov_tril), cov_tril, tf.zeros_like(cov_tril))\n",
        "\n",
        "        num_dim = len(cov_tril.shape)\n",
        "        perm = list(range(num_dim - 2)) + [num_dim - 1, num_dim - 2]\n",
        "        cov_tril_lower = tf.transpose(cov_tril, perm=perm)\n",
        "        z_dist = tfd.MultivariateNormalTriL(loc=mapped_mean, scale_tril=cov_tril_lower)\n",
        "        return z_dist\n",
        "\n",
        "class JointEncoder(tf.keras.Model):\n",
        "    def __init__(self, z_size, hidden_sizes=(64, 64), window_size=3, transpose=False, **kwargs):\n",
        "        \"\"\" Encoder with 1d-convolutional network and factorized Normal posterior\n",
        "            Used by joint VAE and HI-VAE with Standard Normal prior or GP-VAE with factorized Normal posterior\n",
        "            :param z_size: latent space dimensionality\n",
        "            :param hidden_sizes: tuple of hidden layer sizes.\n",
        "                                 The tuple length sets the number of hidden layers.\n",
        "            :param window_size: kernel size for Conv1D layer\n",
        "            :param transpose: True for GP prior | False for Standard Normal prior\n",
        "        \"\"\"\n",
        "        super(JointEncoder, self).__init__()\n",
        "        self.z_size = int(z_size)\n",
        "        self.net = make_cnn(2*z_size, hidden_sizes, window_size)\n",
        "        self.transpose = transpose\n",
        "\n",
        "    def __call__(self, x):\n",
        "        mapped = self.net(x)\n",
        "        if self.transpose:\n",
        "            num_dim = len(x.shape.as_list())\n",
        "            perm = list(range(num_dim - 2)) + [num_dim - 1, num_dim - 2]\n",
        "            mapped = tf.transpose(mapped, perm=perm)\n",
        "            return tfd.MultivariateNormalDiag(\n",
        "                    loc=mapped[..., :self.z_size, :],\n",
        "                    scale_diag=tf.nn.softplus(mapped[..., self.z_size:, :]))\n",
        "        return tfd.MultivariateNormalDiag(\n",
        "                    loc=mapped[..., :self.z_size],\n",
        "                    scale_diag=tf.nn.softplus(mapped[..., self.z_size:]))\n",
        "        \n",
        "class DiagonalEncoder(tf.keras.Model):\n",
        "    def __init__(self, z_size, hidden_sizes=(64, 64), **kwargs):\n",
        "        \"\"\" Encoder with factorized Normal posterior over temporal dimension\n",
        "            Used by disjoint VAE and HI-VAE with Standard Normal prior\n",
        "            :param z_size: latent space dimensionality\n",
        "            :param hidden_sizes: tuple of hidden layer sizes.\n",
        "                                 The tuple length sets the number of hidden layers.\n",
        "        \"\"\"\n",
        "        super(DiagonalEncoder, self).__init__()\n",
        "        self.z_size = int(z_size)\n",
        "        self.net = make_nn(2*z_size, hidden_sizes)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        mapped = self.net(x)\n",
        "        return tfd.MultivariateNormalDiag(\n",
        "          loc=mapped[..., :self.z_size],\n",
        "          scale_diag=tf.nn.softplus(mapped[..., self.z_size:]))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnfAeQAjSrI5"
      },
      "source": [
        "### Decoder ###\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, output_size, hidden_sizes=(64, 64)):\n",
        "        \"\"\" Decoder parent class with no specified output distribution\n",
        "            :param output_size: output dimensionality\n",
        "            :param hidden_sizes: tuple of hidden layer sizes.\n",
        "                                 The tuple length sets the number of hidden layers.\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        self.net = make_nn(output_size, hidden_sizes)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        pass\n",
        "\n",
        "\n",
        "class GaussianDecoder(Decoder):\n",
        "    \"\"\" Decoder with Gaussian output distribution (used for Physionet) \"\"\"\n",
        "    def __call__(self, x):\n",
        "        mean = self.net(x)\n",
        "        var = tf.ones(tf.shape(mean), dtype=tf.float32)\n",
        "        return tfd.Normal(loc=mean, scale=var)\n",
        "\n",
        "class BernoulliDecoder(Decoder):\n",
        "    \"\"\" Decoder with Bernoulli output distribution (used for HMNIST) \"\"\"\n",
        "    def __call__(self, x):\n",
        "        mapped = self.net(x)\n",
        "        return tfd.Bernoulli(logits=mapped)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk8Psy0qWBFa"
      },
      "source": [
        "### GP kernel ###\n",
        "def cauchy_kernel(T, sigma, length_scale):\n",
        "    xs = tf.range(T, dtype=tf.float32)\n",
        "    xs_in = tf.expand_dims(xs, 0)\n",
        "    xs_out = tf.expand_dims(xs, 1)\n",
        "    distance_matrix = tf.math.squared_difference(xs_in, xs_out)\n",
        "    distance_matrix_scaled = distance_matrix / length_scale ** 2\n",
        "    kernel_matrix = tf.math.divide(sigma, (distance_matrix_scaled + 1.))\n",
        "\n",
        "    alpha = 0.001\n",
        "    eye = tf.eye(num_rows=kernel_matrix.shape.as_list()[-1])\n",
        "    return kernel_matrix + alpha * eye\n",
        "\n",
        "\n",
        "def rbf_kernel(T, length_scale):\n",
        "    xs = tf.range(T, dtype=tf.float32)\n",
        "    xs_in = tf.expand_dims(xs, 0)\n",
        "    xs_out = tf.expand_dims(xs, 1)\n",
        "    distance_matrix = tf.math.squared_difference(xs_in, xs_out)\n",
        "    distance_matrix_scaled = distance_matrix / length_scale ** 2\n",
        "    kernel_matrix = tf.math.exp(-distance_matrix_scaled)\n",
        "    return kernel_matrix\n",
        "\n",
        "\n",
        "def diffusion_kernel(T, length_scale):\n",
        "    assert length_scale < 0.5, \"length_scale has to be smaller than 0.5 for the \"\\\n",
        "                               \"kernel matrix to be diagonally dominant\"\n",
        "    sigmas = tf.ones(shape=[T, T]) * length_scale\n",
        "    sigmas_tridiag = tf.linalg.band_part(sigmas, 1, 1)\n",
        "    kernel_matrix = sigmas_tridiag + tf.eye(T)*(1. - length_scale)\n",
        "    return kernel_matrix\n",
        "\n",
        "\n",
        "def matern_kernel(T, length_scale):\n",
        "    xs = tf.range(T, dtype=tf.float32)\n",
        "    xs_in = tf.expand_dims(xs, 0)\n",
        "    xs_out = tf.expand_dims(xs, 1)\n",
        "    distance_matrix = tf.math.abs(xs_in - xs_out)\n",
        "    distance_matrix_scaled = distance_matrix / tf.cast(tf.math.sqrt(length_scale), dtype=tf.float32)\n",
        "    kernel_matrix = tf.math.exp(-distance_matrix_scaled)\n",
        "    return kernel_matrix"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9rMpoVXS7D-"
      },
      "source": [
        "### VAE ###\n",
        "class VAE(tf.keras.Model):\n",
        "    def __init__(self, latent_dim, data_dim, time_length,\n",
        "                 encoder_sizes=(64, 64), encoder=DiagonalEncoder,\n",
        "                 decoder_sizes=(64, 64), decoder=BernoulliDecoder,\n",
        "                 image_preprocessor=None, beta=1.0, M=1, K=1, **kwargs):\n",
        "        \"\"\" Basic Variational Autoencoder with Standard Normal prior\n",
        "            :param latent_dim: latent space dimensionality\n",
        "            :param data_dim: original data dimensionality\n",
        "            :param time_length: time series duration\n",
        "            \n",
        "            :param encoder_sizes: layer sizes for the encoder network\n",
        "            :param encoder: encoder model class {Diagonal, Joint, BandedJoint}Encoder\n",
        "            :param decoder_sizes: layer sizes for the decoder network\n",
        "            :param decoder: decoder model class {Bernoulli, Gaussian}Decoder\n",
        "            \n",
        "            :param image_preprocessor: 2d-convolutional network used for image data preprocessing\n",
        "            :param beta: tradeoff coefficient between reconstruction and KL terms in ELBO\n",
        "            :param M: number of Monte Carlo samples for ELBO estimation\n",
        "            :param K: number of importance weights for IWAE model (see: https://arxiv.org/abs/1509.00519)\n",
        "        \"\"\"\n",
        "        super(VAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.time_length = time_length\n",
        "\n",
        "        self.encoder = encoder(latent_dim, encoder_sizes, **kwargs)\n",
        "        self.decoder = decoder(data_dim, decoder_sizes)\n",
        "        self.preprocessor = image_preprocessor\n",
        "\n",
        "        self.beta = beta\n",
        "        self.K = K\n",
        "        self.M = M\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = tf.identity(x)  # in case x is not a Tensor already...\n",
        "        if self.preprocessor is not None:\n",
        "            x_shape = x.shape.as_list()\n",
        "            new_shape = [x_shape[0] * x_shape[1]] + list(self.preprocessor.image_shape)\n",
        "            x_reshaped = tf.reshape(x, new_shape)\n",
        "            x_preprocessed = self.preprocessor(x_reshaped)\n",
        "            x = tf.reshape(x_preprocessed, x_shape)\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = tf.identity(z)  # in case z is not a Tensor already...\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.decode(self.encode(inputs).sample()).sample()\n",
        "\n",
        "    def generate(self, noise=None, num_samples=1):\n",
        "        if noise is None:\n",
        "            noise = tf.random_normal(shape=(num_samples, self.latent_dim))\n",
        "        return self.decode(noise)\n",
        "    \n",
        "    def _get_prior(self):\n",
        "        if self.prior is None:\n",
        "            self.prior = tfd.MultivariateNormalDiag(loc=tf.zeros(self.latent_dim, dtype=tf.float32),\n",
        "                                                    scale_diag=tf.ones(self.latent_dim, dtype=tf.float32))\n",
        "        return self.prior\n",
        "\n",
        "    def compute_nll(self, x, y=None, m_mask=None):\n",
        "        # Used only for evaluation\n",
        "        assert len(x.shape) == 3, \"Input should have shape: [batch_size, time_length, data_dim]\"\n",
        "        if y is None: y = x\n",
        "\n",
        "        z_sample = self.encode(x).sample()\n",
        "        x_hat_dist = self.decode(z_sample)\n",
        "        nll = -x_hat_dist.log_prob(y)  # shape=(BS, TL, D)\n",
        "        nll = tf.where(tf.math.is_finite(nll), nll, tf.zeros_like(nll))\n",
        "        if m_mask is not None:\n",
        "            m_mask = tf.cast(m_mask, tf.bool)\n",
        "            nll = tf.where(m_mask, nll, tf.zeros_like(nll))  # !!! inverse mask, set zeros for observed\n",
        "        return tf.reduce_sum(nll)\n",
        "\n",
        "    def compute_mse(self, x, y=None, m_mask=None, binary=False):\n",
        "        # Used only for evaluation\n",
        "        assert len(x.shape) == 3, \"Input should have shape: [batch_size, time_length, data_dim]\"\n",
        "        if y is None: y = x\n",
        "\n",
        "        z_mean = self.encode(x).mean()\n",
        "        x_hat_mean = self.decode(z_mean).mean()  # shape=(BS, TL, D)\n",
        "        if binary:\n",
        "            x_hat_mean = tf.round(x_hat_mean)\n",
        "        mse = tf.math.squared_difference(x_hat_mean, y)\n",
        "        if m_mask is not None:\n",
        "            m_mask = tf.cast(m_mask, tf.bool)\n",
        "            mse = tf.where(m_mask, mse, tf.zeros_like(mse))  # !!! inverse mask, set zeros for observed\n",
        "        return tf.reduce_sum(mse)\n",
        "\n",
        "    def _compute_loss(self, x, m_mask=None, return_parts=False):\n",
        "        assert len(x.shape) == 3, \"Input should have shape: [batch_size, time_length, data_dim]\"\n",
        "        x = tf.identity(x)  # in case x is not a Tensor already...\n",
        "        x = tf.tile(x, [self.M * self.K, 1, 1])  # shape=(M*K*BS, TL, D)\n",
        "\n",
        "        if m_mask is not None:\n",
        "            m_mask = tf.identity(m_mask)  # in case m_mask is not a Tensor already...\n",
        "            m_mask = tf.tile(m_mask, [self.M * self.K, 1, 1])  # shape=(M*K*BS, TL, D)\n",
        "            m_mask = tf.cast(m_mask, tf.bool)\n",
        "\n",
        "        pz = self._get_prior()\n",
        "        qz_x = self.encode(x)\n",
        "        z = qz_x.sample()\n",
        "        px_z = self.decode(z)\n",
        "\n",
        "        nll = -px_z.log_prob(x)  # shape=(M*K*BS, TL, D)\n",
        "        nll = tf.where(tf.math.is_finite(nll), nll, tf.zeros_like(nll))\n",
        "        if m_mask is not None:\n",
        "            nll = tf.where(m_mask, tf.zeros_like(nll), nll)  # if not HI-VAE, m_mask is always zeros\n",
        "        nll = tf.reduce_sum(nll, [1, 2])  # shape=(M*K*BS)\n",
        "\n",
        "        if self.K > 1:\n",
        "            kl = qz_x.log_prob(z) - pz.log_prob(z)  # shape=(M*K*BS, TL or d)\n",
        "            kl = tf.where(tf.is_finite(kl), kl, tf.zeros_like(kl))\n",
        "            kl = tf.reduce_sum(kl, 1)  # shape=(M*K*BS)\n",
        "\n",
        "            weights = -nll - kl  # shape=(M*K*BS)\n",
        "            weights = tf.reshape(weights, [self.M, self.K, -1])  # shape=(M, K, BS)\n",
        "\n",
        "            elbo = reduce_logmeanexp(weights, axis=1)  # shape=(M, 1, BS)\n",
        "            elbo = tf.reduce_mean(elbo)  # scalar\n",
        "        else:\n",
        "            # if K==1, compute KL analytically\n",
        "            kl = self.kl_divergence(qz_x, pz)  # shape=(M*K*BS, TL or d)\n",
        "            kl = tf.where(tf.math.is_finite(kl), kl, tf.zeros_like(kl))\n",
        "            kl = tf.reduce_sum(kl, 1)  # shape=(M*K*BS)\n",
        "\n",
        "            elbo = -nll - self.beta * kl  # shape=(M*K*BS) K=1\n",
        "            elbo = tf.reduce_mean(elbo)  # scalar\n",
        "\n",
        "        if return_parts:\n",
        "            nll = tf.reduce_mean(nll)  # scalar\n",
        "            kl = tf.reduce_mean(kl)  # scalar\n",
        "            return -elbo, nll, kl\n",
        "        else:\n",
        "            return -elbo\n",
        "\n",
        "    def compute_loss(self, x, m_mask=None, return_parts=False):\n",
        "        del m_mask\n",
        "        return self._compute_loss(x, return_parts=return_parts)\n",
        "\n",
        "    def kl_divergence(self, a, b):\n",
        "        return tfd.kl_divergence(a, b)\n",
        "\n",
        "    def get_trainable_vars(self):\n",
        "        self.compute_loss(tf.random.normal(shape=(1, self.time_length, self.data_dim), dtype=tf.float32),\n",
        "                          tf.zeros(shape=(1, self.time_length, self.data_dim), dtype=tf.float32))\n",
        "        return self.trainable_variables\n",
        "\n",
        "\n",
        "class HI_VAE(VAE):\n",
        "    \"\"\" HI-VAE model, where the reconstruction term in ELBO is summed only over observed components \"\"\"\n",
        "    def compute_loss(self, x, m_mask=None, return_parts=False):\n",
        "        return self._compute_loss(x, m_mask=m_mask, return_parts=return_parts)\n",
        "\n",
        "\n",
        "class GP_VAE(HI_VAE):\n",
        "    def __init__(self, *args, kernel=\"cauchy\", sigma=1., length_scale=1.0, kernel_scales=1, **kwargs):\n",
        "        \"\"\" Proposed GP-VAE model with Gaussian Process prior\n",
        "            :param kernel: Gaussial Process kernel [\"cauchy\", \"diffusion\", \"rbf\", \"matern\"]\n",
        "            :param sigma: scale parameter for a kernel function\n",
        "            :param length_scale: length scale parameter for a kernel function\n",
        "            :param kernel_scales: number of different length scales over latent space dimensions\n",
        "        \"\"\"\n",
        "        super(GP_VAE, self).__init__(*args, **kwargs)\n",
        "        self.kernel = kernel\n",
        "        self.sigma = sigma\n",
        "        self.length_scale = length_scale\n",
        "        self.kernel_scales = kernel_scales\n",
        "\n",
        "        if isinstance(self.encoder, JointEncoder):\n",
        "            self.encoder.transpose = True\n",
        "\n",
        "        # Precomputed KL components for efficiency\n",
        "        self.pz_scale_inv = None\n",
        "        self.pz_scale_log_abs_determinant = None\n",
        "        self.prior = None\n",
        "\n",
        "    def decode(self, z):\n",
        "        num_dim = len(z.shape)\n",
        "        assert num_dim > 2\n",
        "        perm = list(range(num_dim - 2)) + [num_dim - 1, num_dim - 2]\n",
        "        return self.decoder(tf.transpose(z, perm=perm))\n",
        "\n",
        "    def _get_prior(self):\n",
        "        if self.prior is None:\n",
        "            # Compute kernel matrices for each latent dimension\n",
        "            kernel_matrices = []\n",
        "            for i in range(self.kernel_scales):\n",
        "                if self.kernel == \"rbf\":\n",
        "                    kernel_matrices.append(rbf_kernel(self.time_length, self.length_scale / 2**i))\n",
        "                elif self.kernel == \"diffusion\":\n",
        "                    kernel_matrices.append(diffusion_kernel(self.time_length, self.length_scale / 2**i))\n",
        "                elif self.kernel == \"matern\":\n",
        "                    kernel_matrices.append(matern_kernel(self.time_length, self.length_scale / 2**i))\n",
        "                elif self.kernel == \"cauchy\":\n",
        "                    kernel_matrices.append(cauchy_kernel(self.time_length, self.sigma, self.length_scale / 2**i))\n",
        "\n",
        "            # Combine kernel matrices for each latent dimension\n",
        "            tiled_matrices = []\n",
        "            total = 0\n",
        "            for i in range(self.kernel_scales):\n",
        "                if i == self.kernel_scales-1:\n",
        "                    multiplier = self.latent_dim - total\n",
        "                else:\n",
        "                    multiplier = int(np.ceil(self.latent_dim / self.kernel_scales))\n",
        "                    total += multiplier\n",
        "                tiled_matrices.append(tf.tile(tf.expand_dims(kernel_matrices[i], 0), [multiplier, 1, 1]))\n",
        "            kernel_matrix_tiled = np.concatenate(tiled_matrices)\n",
        "            assert len(kernel_matrix_tiled) == self.latent_dim\n",
        "\n",
        "            self.prior = tfd.MultivariateNormalFullCovariance(\n",
        "                loc=tf.zeros([self.latent_dim, self.time_length], dtype=tf.float32),\n",
        "                covariance_matrix=kernel_matrix_tiled)\n",
        "        return self.prior\n",
        "\n",
        "    def kl_divergence(self, a, b):\n",
        "        \"\"\" Batched KL divergence `KL(a || b)` for multivariate Normals.\n",
        "            See https://github.com/tensorflow/probability/blob/master/tensorflow_probability\n",
        "                       /python/distributions/mvn_linear_operator.py\n",
        "            It's used instead of default KL class in order to exploit precomputed components for efficiency\n",
        "        \"\"\"\n",
        "\n",
        "        def squared_frobenius_norm(x):\n",
        "            \"\"\"Helper to make KL calculation slightly more readable.\"\"\"\n",
        "            return tf.reduce_sum(tf.square(x), axis=[-2, -1])\n",
        "\n",
        "        def is_diagonal(x):\n",
        "            \"\"\"Helper to identify if `LinearOperator` has only a diagonal component.\"\"\"\n",
        "            return (isinstance(x, tf.linalg.LinearOperatorIdentity) or\n",
        "                    isinstance(x, tf.linalg.LinearOperatorScaledIdentity) or\n",
        "                    isinstance(x, tf.linalg.LinearOperatorDiag))\n",
        "\n",
        "        if is_diagonal(a.scale) and is_diagonal(b.scale):\n",
        "            # Using `stddev` because it handles expansion of Identity cases.\n",
        "            b_inv_a = (a.stddev() / b.stddev())[..., tf.newaxis]\n",
        "        else:\n",
        "            if self.pz_scale_inv is None:\n",
        "                self.pz_scale_inv = tf.linalg.inv(b.scale.to_dense())\n",
        "                self.pz_scale_inv = tf.where(tf.math.is_finite(self.pz_scale_inv),\n",
        "                                             self.pz_scale_inv, tf.zeros_like(self.pz_scale_inv))\n",
        "\n",
        "            if self.pz_scale_log_abs_determinant is None:\n",
        "                self.pz_scale_log_abs_determinant = b.scale.log_abs_determinant()\n",
        "\n",
        "            a_shape = a.scale.shape\n",
        "            if len(b.scale.shape) == 3:\n",
        "                _b_scale_inv = tf.tile(self.pz_scale_inv[tf.newaxis], [a_shape[0]] + [1] * (len(a_shape) - 1))\n",
        "            else:\n",
        "                _b_scale_inv = tf.tile(self.pz_scale_inv, [a_shape[0]] + [1] * (len(a_shape) - 1))\n",
        "\n",
        "            b_inv_a = _b_scale_inv @ a.scale.to_dense()\n",
        "\n",
        "        # ~10x times faster on CPU then on GPU\n",
        "        with tf.device('/cpu:0'):\n",
        "            kl_div = (self.pz_scale_log_abs_determinant - a.scale.log_abs_determinant() +\n",
        "                      0.5 * (-tf.cast(a.scale.domain_dimension_tensor(), a.dtype) +\n",
        "                      squared_frobenius_norm(b_inv_a) + squared_frobenius_norm(\n",
        "                      b.scale.solve((b.mean() - a.mean())[..., tf.newaxis]))))\n",
        "        return kl_div"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIpyni_uUGw2",
        "outputId": "d780f2f5-bb0d-4368-d7ba-4183a8813c34"
      },
      "source": [
        "!wget https://www.dropbox.com/s/651d86winb4cy9n/physionet.npz?dl=1 -O physionet.npz"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-30 06:02:24--  https://www.dropbox.com/s/651d86winb4cy9n/physionet.npz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6020:18::a27d:4012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/651d86winb4cy9n/physionet.npz [following]\n",
            "--2021-05-30 06:02:24--  https://www.dropbox.com/s/dl/651d86winb4cy9n/physionet.npz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc09ddf66462d6aee90b10e91b8e.dl.dropboxusercontent.com/cd/0/get/BPemcShlqiYel-E2QJYn0zw67rGVxbVtR5K7Y_33OmQTAtpb4ABNsU4gibuQvQjeHPV7WUa2yzvyNotE4RTrcC6enWDWk6ZsSRDkiSamnyl_rDsw_uxxVk-TwzsBl3OLqq3_hgsO4eMcoNltcL-JBfHj/file?dl=1# [following]\n",
            "--2021-05-30 06:02:25--  https://uc09ddf66462d6aee90b10e91b8e.dl.dropboxusercontent.com/cd/0/get/BPemcShlqiYel-E2QJYn0zw67rGVxbVtR5K7Y_33OmQTAtpb4ABNsU4gibuQvQjeHPV7WUa2yzvyNotE4RTrcC6enWDWk6ZsSRDkiSamnyl_rDsw_uxxVk-TwzsBl3OLqq3_hgsO4eMcoNltcL-JBfHj/file?dl=1\n",
            "Resolving uc09ddf66462d6aee90b10e91b8e.dl.dropboxusercontent.com (uc09ddf66462d6aee90b10e91b8e.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc09ddf66462d6aee90b10e91b8e.dl.dropboxusercontent.com (uc09ddf66462d6aee90b10e91b8e.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322309994 (307M) [application/binary]\n",
            "Saving to: ‘physionet.npz’\n",
            "\n",
            "physionet.npz       100%[===================>] 307.38M   114MB/s    in 2.7s    \n",
            "\n",
            "2021-05-30 06:02:28 (114 MB/s) - ‘physionet.npz’ saved [322309994/322309994]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEqt2-gkXvf1"
      },
      "source": [
        "sys.path.append(\"..\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEAWRMGLYQYQ"
      },
      "source": [
        "### Physionet config ###\n",
        "latent_dim = 35             #Dimensionality of the latent space\n",
        "encoder_sizes = [128, 128]  #Layer sizes of the encoder\n",
        "decoder_sizes = [256, 256]  #Layer sizes of the decoder\n",
        "window_size = 24            #Window size for the inference CNN\n",
        "sigma = 1.005               #Sigma value for the GP prior\n",
        "length_scale = 7            #Length scale value for the GP prior\n",
        "beta = 0.2                  #Factor to weigh the KL term (similar to beta-VAE)\n",
        "num_epochs = 40             #Number of training epochs\n",
        "\n",
        "learning_rate = 1e-3        #Learning rate for training\n",
        "gradient_clip = 1e4         #Maximum global gradient norm for the gradient clipping during training\n",
        "num_steps = 0               #Number of training steps: If non-zero it overwrites num_epochs\n",
        "print_interval = 0                        #Interval for printing the loss and saving the model during training\n",
        "exp_name = \"reproduce_physionet\"          #Name of the experiment\n",
        "basedir = \"models\"                        #Directory where the models should be stored\n",
        "data_dir = \"physionet.npz\" #Directory from where the data should be read in\n",
        "#data/physionet/\n",
        "data_type = 'physionet'                   #Type of data to be trained on\n",
        "seed = 1337                               #Seed for the random number generator\n",
        "model_type = 'gp-vae'       #Type of model to be trained: ['vae', 'hi-vae', 'gp-vae']\n",
        "cnn_kernel_size = 3         #Kernel size for the CNN preprocessor\n",
        "cnn_sizes = [256]           #Number of filters for the layers of the CNN preprocessor\n",
        "testing = True              #Use the actual test set for testing\n",
        "banded_covar = True         #Use a banded covariance matrix instead of a diagonal one for the output of the inference network\n",
        "batch_size = 64             #Batch size for training\n",
        "M = 1                       #Number of samples for ELBO estimation\n",
        "K = 1                       #Number of importance sampling weights\n",
        "kernel = 'cauchy'           #Kernel to be used for the GP prior: ['rbf', 'diffusion', 'matern', 'cauchy']\n",
        "kernel_scales = 1           #Number of different length scales sigma for the GP prior"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an2oVzDocRW2",
        "outputId": "faadb6fa-185e-4779-a950-fd04bf165552"
      },
      "source": [
        "print(\"Testing: \", testing)\n",
        "# Make up full exp name\n",
        "timestamp = datetime.now().strftime(\"%y%m%d\")\n",
        "full_exp_name = \"{}_{}\".format(timestamp, exp_name)\n",
        "outdir = os.path.join(basedir, full_exp_name)\n",
        "#if not os.path.exists(outdir): os.mkdir(outdir)\n",
        "#checkpoint_prefix = os.path.join(outdir, \"ckpt\")\n",
        "print(\"Full exp name: \", full_exp_name)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing:  True\n",
            "Full exp name:  210530_reproduce_physionet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnPK9SRQdw1l"
      },
      "source": [
        "### Define data specific parameters ###\n",
        "data_dim = 35\n",
        "time_length = 48\n",
        "num_classes = 2\n",
        "decoder = GaussianDecoder"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5voYDSrGeHxz"
      },
      "source": [
        "### Load data ###\n",
        "data = np.load(data_dir)\n",
        "x_train_full = data['x_train_full']\n",
        "x_train_miss = data['x_train_miss']\n",
        "m_train_miss = data['m_train_miss']\n",
        "y_train = data['y_train']\n",
        "if testing:\n",
        "    x_val_full = data['x_train_full']\n",
        "    x_val_miss = data['x_train_miss']\n",
        "    m_val_miss = data['m_train_miss']\n",
        "    y_val = data['y_train']\n",
        "    m_val_artificial = data[\"m_train_artificial\"]\n",
        "else:\n",
        "    x_val_full = data[\"x_val_full\"]  # full for artificial missings\n",
        "    x_val_miss = data[\"x_val_miss\"]\n",
        "    m_val_miss = data[\"m_val_miss\"]\n",
        "    m_val_artificial = data[\"m_val_artificial\"]\n",
        "    y_val = data[\"y_val\"]\n",
        "\n",
        "tf_x_train_miss = tf.data.Dataset.from_tensor_slices((x_train_miss, m_train_miss)).shuffle(len(x_train_miss)).batch(batch_size).repeat()\n",
        "tf_x_val_miss = tf.data.Dataset.from_tensor_slices((x_val_miss, m_val_miss)).batch(batch_size).repeat()\n",
        "tf_x_val_miss = tf.compat.v1.data.make_one_shot_iterator(tf_x_val_miss)\n",
        "\n",
        "# Build Conv2D preprocessor for image data\n",
        "image_preprocessor = None"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASR0oQmOf6-P"
      },
      "source": [
        "### Build model ###\n",
        "if model_type == \"vae\":\n",
        "    model = VAE(latent_dim=latent_dim, data_dim=data_dim, time_length=time_length,\n",
        "                encoder_sizes=encoder_sizes, encoder=DiagonalEncoder,\n",
        "                decoder_sizes=decoder_sizes, decoder=decoder,\n",
        "                image_preprocessor=image_preprocessor, window_size=window_size,\n",
        "                beta=beta, M=M, K=K)\n",
        "elif model_type == \"hi-vae\":\n",
        "    model = HI_VAE(latent_dim=latent_dim, data_dim=data_dim, time_length=time_length,\n",
        "                  encoder_sizes=encoder_sizes, encoder=DiagonalEncoder,\n",
        "                  decoder_sizes=decoder_sizes, decoder=decoder,\n",
        "                  image_preprocessor=image_preprocessor, window_size=window_size,\n",
        "                  beta=beta, M=M, K=K)\n",
        "elif model_type == \"gp-vae\":\n",
        "    encoder = BandedJointEncoder if banded_covar else JointEncoder\n",
        "    model = GP_VAE(latent_dim=latent_dim, data_dim=data_dim, time_length=time_length,\n",
        "                  encoder_sizes=encoder_sizes, encoder=encoder,\n",
        "                  decoder_sizes=decoder_sizes, decoder=decoder,\n",
        "                  kernel=kernel, sigma=sigma,\n",
        "                  length_scale=length_scale, kernel_scales = kernel_scales,\n",
        "                  image_preprocessor=image_preprocessor, window_size=window_size,\n",
        "                  beta=beta, M=M, K=K, data_type=data_type)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BTumu3tg5U3",
        "outputId": "2e0577de-d306-4479-c4f4-fe9e885545eb"
      },
      "source": [
        "### Training preparation ###\n",
        "print(\"GPU support: \", tf.test.is_gpu_available())\n",
        "print(\"Training...\")\n",
        "_ = tf.compat.v1.train.get_or_create_global_step()\n",
        "trainable_vars = model.get_trainable_vars()\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "print(\"Encoder: \", model.encoder.net.summary())\n",
        "print(\"Decoder: \", model.decoder.net.summary())\n",
        "\n",
        "if model.preprocessor is not None:\n",
        "    print(\"Preprocessor: \", model.preprocessor.net.summary())\n",
        "    saver = tf.compat.v1.train.Checkpoint(optimizer=optimizer, encoder=model.encoder.net,\n",
        "                                          decoder=model.decoder.net, preprocessor=model.preprocessor.net,\n",
        "                                          optimizer_step=tf.compat.v1.train.get_or_create_global_step())\n",
        "else:\n",
        "    saver = tf.compat.v1.train.Checkpoint(optimizer=optimizer, encoder=model.encoder.net, decoder=model.decoder.net,\n",
        "                                          optimizer_step=tf.compat.v1.train.get_or_create_global_step())\n",
        "\n",
        "#summary_writer = tf.contrib.summary.create_file_writer(outdir, flush_millis=10000)\n",
        "\n",
        "if num_steps == 0:\n",
        "    num_steps = num_epochs * len(x_train_miss) // batch_size\n",
        "else:\n",
        "    num_steps = num_steps\n",
        "\n",
        "if print_interval == 0:\n",
        "    print_interval = num_steps // num_epochs"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU support:  True\n",
            "Training...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              multiple                  107648    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  13545     \n",
            "=================================================================\n",
            "Total params: 137,705\n",
            "Trainable params: 137,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Encoder:  None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              multiple                  9216      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  65792     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              multiple                  8995      \n",
            "=================================================================\n",
            "Total params: 84,003\n",
            "Trainable params: 84,003\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Decoder:  None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_VIxT-khuJb",
        "outputId": "02776df9-3fd3-4064-f9bd-e2ed5dff0ccb"
      },
      "source": [
        "### Training ###\n",
        "losses_train = []\n",
        "losses_val = []\n",
        "\n",
        "t0 = time.time()\n",
        "#with summary_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
        "for i, (x_seq, m_seq) in enumerate(tf_x_train_miss.take(num_steps)):\n",
        "    try:\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(trainable_vars)\n",
        "            loss = model.compute_loss(x_seq, m_mask=m_seq)\n",
        "            losses_train.append(loss.numpy())\n",
        "        grads = tape.gradient(loss, trainable_vars)\n",
        "        grads = [np.nan_to_num(grad) for grad in grads]\n",
        "        grads, global_norm = tf.clip_by_global_norm(grads, gradient_clip)\n",
        "        optimizer.apply_gradients(zip(grads, trainable_vars),\n",
        "                                  global_step=tf.compat.v1.train.get_or_create_global_step())\n",
        "\n",
        "        # Print intermediate results\n",
        "        if i % print_interval == 0:\n",
        "            print(\"================================================\")\n",
        "            print(\"Learning rate: {} | Global gradient norm: {:.2f}\".format(optimizer._lr, global_norm))\n",
        "            print(\"Step {}) Time = {:2f}\".format(i, time.time() - t0))\n",
        "            loss, nll, kl = model.compute_loss(x_seq, m_mask=m_seq, return_parts=True)\n",
        "            print(\"Train loss = {:.3f} | NLL = {:.3f} | KL = {:.3f}\".format(loss, nll, kl))\n",
        "\n",
        "            #saver.save(checkpoint_prefix)\n",
        "            tf.contrib.summary.scalar(\"loss_train\", loss)\n",
        "            tf.contrib.summary.scalar(\"kl_train\", kl)\n",
        "            tf.contrib.summary.scalar(\"nll_train\", nll)\n",
        "\n",
        "            # Validation loss\n",
        "            x_val_batch, m_val_batch = tf_x_val_miss.get_next()\n",
        "            val_loss, val_nll, val_kl = model.compute_loss(x_val_batch, m_mask=m_val_batch, return_parts=True)\n",
        "            losses_val.append(val_loss.numpy())\n",
        "            print(\"Validation loss = {:.3f} | NLL = {:.3f} | KL = {:.3f}\".format(val_loss, val_nll, val_kl))\n",
        "\n",
        "            tf.contrib.summary.scalar(\"loss_val\", val_loss)\n",
        "            tf.contrib.summary.scalar(\"kl_val\", val_kl)\n",
        "            tf.contrib.summary.scalar(\"nll_val\", val_nll)\n",
        "\n",
        "            # Eval MSE and AUROC on entire val set\n",
        "            x_val_miss_batches = np.array_split(x_val_miss, batch_size, axis=0)\n",
        "            x_val_full_batches = np.array_split(x_val_full, batch_size, axis=0)\n",
        "            m_val_artificial_batches = np.array_split(m_val_artificial, batch_size, axis=0)\n",
        "            get_val_batches = lambda: zip(x_val_miss_batches, x_val_full_batches, m_val_artificial_batches)\n",
        "\n",
        "            n_missings = m_val_artificial.sum()\n",
        "            mse_miss = np.sum([model.compute_mse(x, y=y, m_mask=m).numpy()\n",
        "                              for x, y, m in get_val_batches()]) / n_missings\n",
        "\n",
        "            x_val_imputed = np.vstack([model.decode(model.encode(x_batch).mean()).mean().numpy()\n",
        "                                      for x_batch in x_val_miss_batches])\n",
        "            x_val_imputed[m_val_miss == 0] = x_val_miss[m_val_miss == 0]  # impute gt observed values\n",
        "\n",
        "            x_val_imputed = x_val_imputed.reshape([-1, time_length * data_dim])\n",
        "            val_split = len(x_val_imputed) // 2\n",
        "            cls_model = LogisticRegression(solver='liblinear', tol=1e-10, max_iter=10000)\n",
        "            cls_model.fit(x_val_imputed[:val_split], y_val[:val_split])\n",
        "            probs = cls_model.predict_proba(x_val_imputed[val_split:])[:, 1]\n",
        "            auroc = roc_auc_score(y_val[val_split:], probs)\n",
        "            print(\"MSE miss: {:.4f} | AUROC: {:.4f}\".format(mse_miss, auroc))\n",
        "\n",
        "            # Update learning rate (used only for physionet with decay=0.5)\n",
        "            if i > 0 and i % (10*print_interval) == 0:\n",
        "                optimizer._lr = max(0.5 * optimizer._lr, 0.1 * learning_rate)\n",
        "            t0 = time.time()\n",
        "    except KeyboardInterrupt:\n",
        "        saver.save(checkpoint_prefix)\n",
        "        break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 9210.20\n",
            "Step 0) Time = 0.829310\n",
            "Train loss = 59730.324 | NLL = 429.608 | KL = 296503.562\n",
            "Validation loss = 59727.980 | NLL = 420.049 | KL = 296539.656\n",
            "MSE miss: 1.0452 | AUROC: 0.6679\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 865.38\n",
            "Step 62) Time = 18.122163\n",
            "Train loss = 22424.811 | NLL = 360.015 | KL = 110323.969\n",
            "Validation loss = 22240.102 | NLL = 357.331 | KL = 109413.844\n",
            "MSE miss: 0.7376 | AUROC: 0.6791\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 686.85\n",
            "Step 124) Time = 18.355134\n",
            "Train loss = 22102.387 | NLL = 365.026 | KL = 108686.805\n",
            "Validation loss = 21772.900 | NLL = 372.445 | KL = 107002.266\n",
            "MSE miss: 0.7033 | AUROC: 0.6868\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 344.53\n",
            "Step 186) Time = 18.046797\n",
            "Train loss = 21754.930 | NLL = 358.799 | KL = 106980.656\n",
            "Validation loss = 21669.465 | NLL = 340.465 | KL = 106645.000\n",
            "MSE miss: 0.6921 | AUROC: 0.6915\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 463.13\n",
            "Step 248) Time = 18.044888\n",
            "Train loss = 21676.676 | NLL = 351.944 | KL = 106623.656\n",
            "Validation loss = 21598.195 | NLL = 381.305 | KL = 106084.453\n",
            "MSE miss: 0.6800 | AUROC: 0.7025\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 212.00\n",
            "Step 310) Time = 17.917878\n",
            "Train loss = 21578.258 | NLL = 373.984 | KL = 106021.359\n",
            "Validation loss = 21614.344 | NLL = 360.852 | KL = 106267.469\n",
            "MSE miss: 0.6593 | AUROC: 0.7031\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 600.21\n",
            "Step 372) Time = 18.074188\n",
            "Train loss = 21563.293 | NLL = 357.365 | KL = 106029.641\n",
            "Validation loss = 21567.240 | NLL = 364.958 | KL = 106011.406\n",
            "MSE miss: 0.6528 | AUROC: 0.7058\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 190.25\n",
            "Step 434) Time = 17.990233\n",
            "Train loss = 21572.738 | NLL = 364.982 | KL = 106038.789\n",
            "Validation loss = 21553.043 | NLL = 348.765 | KL = 106021.391\n",
            "MSE miss: 0.6719 | AUROC: 0.7073\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 148.18\n",
            "Step 496) Time = 18.012553\n",
            "Train loss = 21555.561 | NLL = 364.667 | KL = 105954.461\n",
            "Validation loss = 21527.332 | NLL = 332.967 | KL = 105971.812\n",
            "MSE miss: 0.6327 | AUROC: 0.7039\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 323.19\n",
            "Step 558) Time = 18.093580\n",
            "Train loss = 21551.604 | NLL = 359.722 | KL = 105959.406\n",
            "Validation loss = 21572.098 | NLL = 376.961 | KL = 105975.688\n",
            "MSE miss: 0.6483 | AUROC: 0.7043\n",
            "================================================\n",
            "Learning rate: 0.001 | Global gradient norm: 190.98\n",
            "Step 620) Time = 17.959839\n",
            "Train loss = 21539.391 | NLL = 353.186 | KL = 105931.031\n",
            "Validation loss = 21611.273 | NLL = 399.683 | KL = 106057.945\n",
            "MSE miss: 0.6227 | AUROC: 0.7047\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 172.76\n",
            "Step 682) Time = 17.948070\n",
            "Train loss = 21564.781 | NLL = 367.747 | KL = 105985.164\n",
            "Validation loss = 21542.160 | NLL = 359.903 | KL = 105911.281\n",
            "MSE miss: 0.6131 | AUROC: 0.7107\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 255.59\n",
            "Step 744) Time = 17.979035\n",
            "Train loss = 21519.348 | NLL = 338.621 | KL = 105903.625\n",
            "Validation loss = 21512.363 | NLL = 331.195 | KL = 105905.836\n",
            "MSE miss: 0.6164 | AUROC: 0.7081\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 149.76\n",
            "Step 806) Time = 17.987764\n",
            "Train loss = 21521.508 | NLL = 337.232 | KL = 105921.375\n",
            "Validation loss = 21541.316 | NLL = 357.337 | KL = 105919.906\n",
            "MSE miss: 0.6072 | AUROC: 0.7088\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 174.35\n",
            "Step 868) Time = 17.934972\n",
            "Train loss = 21528.518 | NLL = 348.629 | KL = 105899.438\n",
            "Validation loss = 21529.352 | NLL = 345.680 | KL = 105918.352\n",
            "MSE miss: 0.6045 | AUROC: 0.7076\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 131.65\n",
            "Step 930) Time = 18.134897\n",
            "Train loss = 21528.836 | NLL = 349.747 | KL = 105895.445\n",
            "Validation loss = 21533.902 | NLL = 349.480 | KL = 105922.117\n",
            "MSE miss: 0.5985 | AUROC: 0.7097\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 110.20\n",
            "Step 992) Time = 18.004988\n",
            "Train loss = 21525.855 | NLL = 344.273 | KL = 105907.906\n",
            "Validation loss = 21525.725 | NLL = 341.380 | KL = 105921.727\n",
            "MSE miss: 0.5975 | AUROC: 0.7068\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 226.39\n",
            "Step 1054) Time = 18.155917\n",
            "Train loss = 21508.543 | NLL = 330.414 | KL = 105890.641\n",
            "Validation loss = 21529.168 | NLL = 347.529 | KL = 105908.188\n",
            "MSE miss: 0.6028 | AUROC: 0.7078\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 215.76\n",
            "Step 1116) Time = 18.038211\n",
            "Train loss = 21531.262 | NLL = 350.750 | KL = 105902.555\n",
            "Validation loss = 21525.809 | NLL = 347.423 | KL = 105891.922\n",
            "MSE miss: 0.5960 | AUROC: 0.7108\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 326.86\n",
            "Step 1178) Time = 18.156315\n",
            "Train loss = 21533.516 | NLL = 353.544 | KL = 105899.844\n",
            "Validation loss = 21528.438 | NLL = 348.140 | KL = 105901.484\n",
            "MSE miss: 0.5952 | AUROC: 0.7072\n",
            "================================================\n",
            "Learning rate: 0.0005 | Global gradient norm: 138.11\n",
            "Step 1240) Time = 17.979367\n",
            "Train loss = 21513.818 | NLL = 333.859 | KL = 105899.797\n",
            "Validation loss = 21517.020 | NLL = 337.730 | KL = 105896.453\n",
            "MSE miss: 0.5893 | AUROC: 0.7109\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 265.39\n",
            "Step 1302) Time = 17.984462\n",
            "Train loss = 21588.801 | NLL = 404.504 | KL = 105921.484\n",
            "Validation loss = 21514.027 | NLL = 336.276 | KL = 105888.766\n",
            "MSE miss: 0.5874 | AUROC: 0.7093\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 114.18\n",
            "Step 1364) Time = 18.085844\n",
            "Train loss = 21516.473 | NLL = 336.702 | KL = 105898.844\n",
            "Validation loss = 21551.213 | NLL = 368.141 | KL = 105915.359\n",
            "MSE miss: 0.5847 | AUROC: 0.7116\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 174.52\n",
            "Step 1426) Time = 18.071743\n",
            "Train loss = 21505.252 | NLL = 328.733 | KL = 105882.594\n",
            "Validation loss = 21513.238 | NLL = 335.852 | KL = 105886.930\n",
            "MSE miss: 0.5889 | AUROC: 0.7098\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 101.03\n",
            "Step 1488) Time = 18.052821\n",
            "Train loss = 21507.457 | NLL = 330.205 | KL = 105886.266\n",
            "Validation loss = 21542.420 | NLL = 360.434 | KL = 105909.922\n",
            "MSE miss: 0.5812 | AUROC: 0.7105\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 171.15\n",
            "Step 1550) Time = 17.972563\n",
            "Train loss = 21519.592 | NLL = 341.454 | KL = 105890.688\n",
            "Validation loss = 21542.520 | NLL = 364.077 | KL = 105892.211\n",
            "MSE miss: 0.5820 | AUROC: 0.7079\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 234.49\n",
            "Step 1612) Time = 17.959138\n",
            "Train loss = 21523.881 | NLL = 344.991 | KL = 105894.445\n",
            "Validation loss = 21531.961 | NLL = 352.766 | KL = 105895.984\n",
            "MSE miss: 0.5831 | AUROC: 0.7092\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 93.66\n",
            "Step 1674) Time = 18.077527\n",
            "Train loss = 21530.836 | NLL = 351.438 | KL = 105896.992\n",
            "Validation loss = 21521.242 | NLL = 341.394 | KL = 105899.242\n",
            "MSE miss: 0.5817 | AUROC: 0.7035\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 80.99\n",
            "Step 1736) Time = 18.028870\n",
            "Train loss = 21505.633 | NLL = 327.351 | KL = 105891.406\n",
            "Validation loss = 21519.969 | NLL = 341.701 | KL = 105891.344\n",
            "MSE miss: 0.5812 | AUROC: 0.7100\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 152.02\n",
            "Step 1798) Time = 18.017781\n",
            "Train loss = 21526.039 | NLL = 346.595 | KL = 105897.219\n",
            "Validation loss = 21510.980 | NLL = 331.627 | KL = 105896.766\n",
            "MSE miss: 0.5829 | AUROC: 0.7105\n",
            "================================================\n",
            "Learning rate: 0.00025 | Global gradient norm: 124.66\n",
            "Step 1860) Time = 18.111964\n",
            "Train loss = 21504.137 | NLL = 328.050 | KL = 105880.438\n",
            "Validation loss = 21577.867 | NLL = 397.062 | KL = 105904.031\n",
            "MSE miss: 0.5773 | AUROC: 0.7128\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 102.71\n",
            "Step 1922) Time = 18.065661\n",
            "Train loss = 21492.148 | NLL = 313.153 | KL = 105894.977\n",
            "Validation loss = 21507.070 | NLL = 331.181 | KL = 105879.438\n",
            "MSE miss: 0.5771 | AUROC: 0.7142\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 102.52\n",
            "Step 1984) Time = 18.094049\n",
            "Train loss = 21509.246 | NLL = 331.774 | KL = 105887.359\n",
            "Validation loss = 21514.049 | NLL = 336.018 | KL = 105890.148\n",
            "MSE miss: 0.5789 | AUROC: 0.7125\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 111.64\n",
            "Step 2046) Time = 18.043572\n",
            "Train loss = 21505.631 | NLL = 328.673 | KL = 105884.781\n",
            "Validation loss = 21494.566 | NLL = 316.406 | KL = 105890.805\n",
            "MSE miss: 0.5747 | AUROC: 0.7145\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 87.97\n",
            "Step 2108) Time = 18.102128\n",
            "Train loss = 21520.672 | NLL = 343.011 | KL = 105888.297\n",
            "Validation loss = 21506.391 | NLL = 329.376 | KL = 105885.062\n",
            "MSE miss: 0.5763 | AUROC: 0.7094\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 103.71\n",
            "Step 2170) Time = 18.146133\n",
            "Train loss = 21546.840 | NLL = 367.309 | KL = 105897.641\n",
            "Validation loss = 21520.025 | NLL = 341.878 | KL = 105890.734\n",
            "MSE miss: 0.5751 | AUROC: 0.7137\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 85.89\n",
            "Step 2232) Time = 18.113290\n",
            "Train loss = 21507.145 | NLL = 330.536 | KL = 105883.031\n",
            "Validation loss = 21515.193 | NLL = 336.242 | KL = 105894.750\n",
            "MSE miss: 0.5732 | AUROC: 0.7134\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 125.04\n",
            "Step 2294) Time = 18.032108\n",
            "Train loss = 21522.699 | NLL = 344.734 | KL = 105889.828\n",
            "Validation loss = 21524.361 | NLL = 346.315 | KL = 105890.227\n",
            "MSE miss: 0.5739 | AUROC: 0.7158\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 108.01\n",
            "Step 2356) Time = 18.074889\n",
            "Train loss = 21508.418 | NLL = 330.423 | KL = 105889.969\n",
            "Validation loss = 21517.438 | NLL = 339.747 | KL = 105888.453\n",
            "MSE miss: 0.5735 | AUROC: 0.7164\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 94.92\n",
            "Step 2418) Time = 17.944571\n",
            "Train loss = 21502.439 | NLL = 325.561 | KL = 105884.391\n",
            "Validation loss = 21503.324 | NLL = 325.174 | KL = 105890.742\n",
            "MSE miss: 0.5708 | AUROC: 0.7166\n",
            "================================================\n",
            "Learning rate: 0.000125 | Global gradient norm: 124.02\n",
            "Step 2480) Time = 18.033093\n",
            "Train loss = 21514.033 | NLL = 335.871 | KL = 105890.812\n",
            "Validation loss = 21507.934 | NLL = 331.274 | KL = 105883.297\n",
            "MSE miss: 0.5704 | AUROC: 0.7166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6s4vX6HVG6k",
        "outputId": "6055d034-ef80-41f3-d312-38a00a8b63ff"
      },
      "source": [
        "### Evaluation ###\n",
        "print(\"Evaluation...\")\n",
        "\n",
        "# Split data on batches\n",
        "x_val_miss_batches = np.array_split(x_val_miss, batch_size, axis=0)\n",
        "x_val_full_batches = np.array_split(x_val_full, batch_size, axis=0)\n",
        "m_val_batches = np.array_split(m_val_artificial, batch_size, axis=0)\n",
        "\n",
        "get_val_batches = lambda: zip(x_val_miss_batches, x_val_full_batches, m_val_batches)\n",
        "\n",
        "# Compute NLL and MSE on missing values\n",
        "n_missings = m_val_artificial.sum()\n",
        "nll_miss = np.sum([model.compute_nll(x, y=y, m_mask=m).numpy()\n",
        "                  for x, y, m in get_val_batches()]) / n_missings\n",
        "mse_miss = np.sum([model.compute_mse(x, y=y, m_mask=m, binary=False).numpy()\n",
        "                  for x, y, m in get_val_batches()]) / n_missings\n",
        "print(\"NLL miss: {:.4f}\".format(nll_miss))\n",
        "print(\"MSE miss: {:.4f}\".format(mse_miss))\n",
        "\n",
        "# Save imputed values\n",
        "z_mean = [model.encode(x_batch).mean().numpy() for x_batch in x_val_miss_batches]\n",
        "np.save(\"z_mean\", np.vstack(z_mean))\n",
        "x_val_imputed = np.vstack([model.decode(z_batch).mean().numpy() for z_batch in z_mean])\n",
        "np.save(\"imputed_no_gt\", x_val_imputed)\n",
        "\n",
        "# impute gt observed values\n",
        "x_val_imputed[m_val_miss == 0] = x_val_miss[m_val_miss == 0]\n",
        "np.save(\"imputed\", x_val_imputed)\n",
        "\n",
        "# AUROC evaluation using Logistic Regression\n",
        "x_val_imputed = x_val_imputed.reshape([-1, time_length * data_dim])\n",
        "val_split = len(x_val_imputed) // 2\n",
        "cls_model = LogisticRegression(solver='liblinear', tol=1e-10, max_iter=10000)\n",
        "cls_model.fit(x_val_imputed[:val_split], y_val[:val_split])\n",
        "probs = cls_model.predict_proba(x_val_imputed[val_split:])[:, 1]\n",
        "auprc = average_precision_score(y_val[val_split:], probs)\n",
        "auroc = roc_auc_score(y_val[val_split:], probs)\n",
        "\n",
        "print(\"AUROC: {:.4f}\".format(auroc))\n",
        "print(\"AUPRC: {:.4f}\".format(auprc))\n",
        "\n",
        "results_all = [seed, model_type, data_type, kernel, beta, latent_dim,\n",
        "              num_epochs, batch_size, learning_rate, window_size,\n",
        "              kernel_scales, sigma, length_scale,\n",
        "              len(encoder_sizes), encoder_sizes[0] if len(encoder_sizes) > 0 else 0,\n",
        "              len(decoder_sizes), decoder_sizes[0] if len(decoder_sizes) > 0 else 0,\n",
        "              cnn_kernel_size, cnn_sizes,\n",
        "              nll_miss, mse_miss, losses_train[-1], losses_val[-1], auprc, auroc, testing, data_dir]\n",
        "\n",
        "with open(\"training_curve.tsv\", \"w\") as outfile:\n",
        "    outfile.write(\"\\t\".join(map(str, losses_train)))\n",
        "    outfile.write(\"\\n\")\n",
        "    outfile.write(\"\\t\".join(map(str, losses_val)))\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation...\n",
            "NLL miss: 1.2334\n",
            "MSE miss: 0.5753\n",
            "AUROC: 0.7169\n",
            "AUPRC: 0.3226\n",
            "Training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRPWO-DzJe1N",
        "outputId": "99fb10f0-07b5-4496-c029-b8cc4d9cbd77"
      },
      "source": [
        "results = pd.read_csv(\"training_curve.tsv\", sep=\"\\t\")\n",
        "results.shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2498)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeEfkpqmNcvS",
        "outputId": "83fd2550-1bff-4661-dab5-bdf99083d2c6"
      },
      "source": [
        "imputed = np.load('imputed.npy')\n",
        "imputed_no_gt = np.load('imputed_no_gt.npy')\n",
        "print(imputed_no_gt[0, :, 4])\n",
        "print(imputed[0, :, 4])\n",
        "#print(len(x_val_miss_batches[0][0]))"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8444435  0.89018697 0.92373925 0.9519598  0.967882   0.94593\n",
            " 0.8691287  0.74129874 0.61668575 0.54210764 0.5462877  0.6220456\n",
            " 0.7412182  0.8535363  0.9425337  1.0106984  1.0825626  1.1840339\n",
            " 1.3511266  1.5633404  1.8215219  2.0545404  2.2530775  2.3886547\n",
            " 2.4511638  2.4389422  2.3574576  2.2253942  2.0592732  1.916267\n",
            " 1.819739   1.82369    1.915514   2.0819397  2.2527251  2.3837953\n",
            " 2.46572    2.4871123  2.4628167  2.391918   2.3078268  2.1968963\n",
            " 2.0840664  1.9649191  1.8291498  1.6657128  1.4751636  1.2488028 ]\n",
            "[ 2.0491464   1.0531795  -0.07558286  0.19000828  1.7835552   1.517964\n",
            "  1.5843618   0.74129874  0.45559943  0.54210764 -0.54036736  0.6220456\n",
            "  1.9827485   0.8535363   1.3851684   1.0106984   1.9163507   1.1840339\n",
            "  1.9163507   1.5633404   2.5803287   2.2483397   2.5803287   2.4475331\n",
            "  2.9123175   2.6467264   2.6467264   2.8459198   2.0592732   2.3147376\n",
            " -0.8059585   2.2483397   1.915514    1.8499529   2.9123175   2.5139308\n",
            "  2.3811352   2.6467264   2.5139308   2.7131243   2.779522    1.8499529\n",
            "  1.6507596   1.7835552   2.115544    1.6657128   2.3811352   2.779522  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "MF5G0vypKSUk",
        "outputId": "ef8e6848-81d9-4873-807a-03b0c4700d1f"
      },
      "source": [
        "man = randint(0, imputed.shape[0]-1)\n",
        "health_param = randint(0, imputed.shape[2]-1)\n",
        "print(f'Patient {man}, health parameter {health_param}')\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(imputed[man,:, health_param], linestyle='--', marker='o', color='green', label='imputed ground truth values')\n",
        "ax.plot(imputed_no_gt[man,:, health_param], linestyle='-', marker='+', color='blue', label='imputed values')\n",
        "ax.set(xlabel='time (h)', title='Data imputation')\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "plt.show()\n",
        "fig.savefig(\"Imputation.png\")"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Patient 2261, health parameter 23\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}